{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKDUH8YgTV2yl3JSV2FoA3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deven10103/Custom-Embedding-Model/blob/main/Custom%20CBOW%20and%20Skip-gram%20Models%20using%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and setting up environment"
      ],
      "metadata": {
        "id": "ZyekojOXHS0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e_uLNwJRHHrr"
      },
      "outputs": [],
      "source": [
        "!pip install numpy torch transformers\n",
        "!pip install re collections random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import re\n",
        "from collections import Counter,defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "z1DsHkceLhLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up **Tokenizer**"
      ],
      "metadata": {
        "id": "JKBeHRsnQwWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer():\n",
        "  try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    return tokenizer\n",
        "  except:\n",
        "    print(\"WordPeice tokenizer not aavilable.\")\n"
      ],
      "metadata": {
        "id": "AAymsuohMRs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "9r1qWp5fRewy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(texts,tokenizer):\n",
        "  vocab={}\n",
        "  words=[]\n",
        "\n",
        "  for text in texts:\n",
        "    tokens=tokenizer.encode(text,add_special_tokens=False)\n",
        "    for token in tokens:\n",
        "      if token not in vocab:\n",
        "        word=tokenizer.decode([token])\n",
        "        if word not in vocab:\n",
        "            vocab[word]=len(vocab)\n",
        "            words.append(word)\n",
        "\n",
        "  return vocab,words\n",
        "\n",
        "def prepare_training_data(texts,tokenizer, vocab,window_size=2):\n",
        "  cbow_data=[]\n",
        "  skipgram_data=[]\n",
        "\n",
        "  id_to_word={id : tokenizer.decode([id]) for id in tokenizer.vocab.values()}\n",
        "\n",
        "  for text in texts:\n",
        "    tokens=tokenizer.encode(text,add_special_tokens=False)\n",
        "    indexed_tokens=[vocab[id_to_word[token_id]] for token_id in tokens if token_id in id_to_word and id_to_word[token_id] in vocab]\n",
        "\n",
        "    if len(indexed_tokens)<2*window_size+1:\n",
        "      continue\n",
        "\n",
        "    for i in range(window_size,len(indexed_tokens)-window_size):\n",
        "      target=indexed_tokens[i]\n",
        "      context=indexed_tokens[i-window_size:i]+indexed_tokens[i+1:i+window_size+1]\n",
        "\n",
        "      cbow_data.append((context,target))\n",
        "\n",
        "      for ctx_word in context:\n",
        "        skipgram_data.append((target,ctx_word))\n",
        "\n",
        "  return cbow_data,skipgram_data"
      ],
      "metadata": {
        "id": "FLyxqSa_RaS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CBOW** model using Python"
      ],
      "metadata": {
        "id": "qL1fJAXz7coW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWModel(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim):\n",
        "    super(CBOWModel,self).__init__()\n",
        "    self.embeddings=nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.linear=nn.Linear(embedding_dim,vocab_size)\n",
        "\n",
        "  def forward(self,context):\n",
        "    #context: (batch_size,context_size)\n",
        "\n",
        "    embeds=self.embeddings(context)\n",
        "    #embeds: (batch_size,context_size,embedding_dim)\n",
        "\n",
        "    context_vec = torch.mean(embeds, dim=1)\n",
        "    #context_vec: (batch_size,embedding_dim)\n",
        "\n",
        "    output=self.linear(context_vec)\n",
        "    #output: (batch_size,vocab_size)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "hlV1VJPJ7cIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Skipgram** model using Python"
      ],
      "metadata": {
        "id": "T5a6ppsT7lPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim):\n",
        "    super(SkipGram,self).__init__()\n",
        "    self.embeddings=nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.linear=nn.Linear(embedding_dim,vocab_size)\n",
        "\n",
        "  def forward(self,target):\n",
        "    #target: (batch_size)\n",
        "\n",
        "    embed=self.embeddings(target)\n",
        "    #embed: (batch_size,embedding_dim)\n",
        "\n",
        "    output=self.linear(embed)\n",
        "    #output: (batch_size,vocab_size)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "PZqzAoN57kpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prepearation"
      ],
      "metadata": {
        "id": "JbUaBWwZz3z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, data, model_type='cbow'):\n",
        "        self.data = data\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.model_type == 'cbow':\n",
        "            context, target = self.data[idx]\n",
        "            return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "        else:\n",
        "            target, context = self.data[idx]\n",
        "            return torch.tensor(target, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
        "\n",
        "def train_pytorch_model(model, train_loader, epochs=5, lr=0.01):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs): # Corrected variable name from epochs to epoch\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "oOG0FUnwz5yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation** Function"
      ],
      "metadata": {
        "id": "LUPxWazjbNVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_demo(use_larger_dataset=False):\n",
        "\n",
        "  sample_texts = [\n",
        "      \"Lower newer wider space\",\n",
        "      \"The cat sat on the mat\",\n",
        "      \"The quick brown fox jumps over the lazy dog\",\n",
        "      \"Natural language processing is a fascinating filed of study\",\n",
        "      \"Machine Learning Models can learn word embeddings from text\",\n",
        "      \"Word vectors capture semantic ralationships between words\",\n",
        "      \"Deep learning has revilutionized natural language processing\"\n",
        "  ]\n",
        "\n",
        "  if use_larger_dataset:\n",
        "    try:\n",
        "      # Load from a file\n",
        "      #with open('file_name.txt','r',encoding='utf-8') as f:\n",
        "      #   texts = f.readLines()\n",
        "\n",
        "      # Load from a library\n",
        "      #from datasets import load_datasets\n",
        "      #dataset = load_dataset('dataset name')\n",
        "      #texts = [item['text']] for item in dataset[['train']]\n",
        "\n",
        "      texts = sample_texts * 100\n",
        "    except Exception as e:\n",
        "          print(\"Error loading larger dataset: {e}, Using sample dataset.\")\n",
        "          texts = sample_texts\n",
        "  else:\n",
        "    texts = sample_texts\n",
        "\n",
        "\n",
        "  tokenizer = get_tokenizer()\n",
        "\n",
        "  vocab,words = build_vocab(texts,tokenizer)\n",
        "  vocab_size = len(vocab)\n",
        "  embedding_dim = 20;\n",
        "\n",
        "  cbow_data,skipgram_data = prepare_training_data(texts,tokenizer,vocab)\n",
        "\n",
        "  print(\"Pytorch Models Training\")\n",
        "\n",
        "  print(\" Training PyTorch CBOW Model\")\n",
        "  cbow_model = CBOWModel(vocab_size,embedding_dim)\n",
        "  cbow_dataset = Word2VecDataset(cbow_data,model_type='cbow')\n",
        "  cbow_loader = DataLoader(cbow_dataset,batch_size=16,shuffle=True)\n",
        "\n",
        "  trained_cbow = train_pytorch_model(cbow_model,cbow_loader,epochs=10)\n",
        "\n",
        "  print(\" Training PyTorch Skip-gram model\")\n",
        "  skipgram_model = SkipGram(vocab_size,embedding_dim)\n",
        "  skipgram_dataset = Word2VecDataset(skipgram_data,model_type='skipgram')\n",
        "  skipgram_loader = DataLoader(skipgram_dataset,batch_size=16,shuffle=True)\n",
        "\n",
        "  trained_skipgram = train_pytorch_model(skipgram_model,skipgram_loader,epochs=10)\n",
        "\n",
        "  print(\"Model Architecture and Embeddings\")\n",
        "\n",
        "  print(\" CBOW Model Architecture\")\n",
        "  print(trained_cbow)\n",
        "\n",
        "  print(\" Skip-gram Model Embeddings\")\n",
        "  print(trained_skipgram)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if vocab_size > 0:\n",
        "      words_to_print = words[:min(2,vocab_size)]\n",
        "      print(f\"Words: {words_to_print}\")\n",
        "\n",
        "      sample_embeddings_cbow = trained_cbow.embeddings.weight[:min(2,vocab_size)]\n",
        "      print(f\"Shape: {sample_embeddings_cbow.shape}\")\n",
        "      print(f\"Sample Values: {sample_embeddings_cbow}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if vocab_size > 0:\n",
        "      words_to_print = words[:min(2,vocab_size)]\n",
        "      print(f\"Words: {words_to_print}\")\n",
        "\n",
        "      sample_embeddings_skipgram = trained_skipgram.embeddings.weight[:min(2,vocab_size)]\n",
        "      print(f\"Shape: {sample_embeddings_cbow.shape}\")\n",
        "      print(f\"Sample Values: {sample_embeddings_skipgram}\")\n",
        "\n",
        "    print(\"CBOW Prediction:\")\n",
        "\n",
        "  context_words = ['word', 'vectors' ,'semantic',\"between\"]\n",
        "  context_words_lower=[word.lower() for word in context_words]\n",
        "  context_words=context_words_lower\n",
        "  try:\n",
        "    context_indices = [vocab[word] for word in context_words]\n",
        "    if len(context_words) == len(context_indices):\n",
        "      context_tensor = torch.tensor([context_indices],dtype=torch.long)\n",
        "      with torch.no_grad():\n",
        "        output = trained_cbow(context_tensor)\n",
        "        predicted_idx = torch.argmax(output,dim=1).item()\n",
        "        predicted_word = words[predicted_idx]\n",
        "        print(f\"Context: {context_words}, Predicted Target Word: {predicted_word}\")\n",
        "\n",
        "    else:\n",
        "      print(\"One or more context words not found in vocabullary.\")\n",
        "\n",
        "  except KeyError:\n",
        "    print(\"One or more context words not found in vocabullary.\")\n",
        "\n",
        "\n",
        "  print(\"Skip-gram Prediction:\")\n",
        "  target_word = \"capture\"\n",
        "  target_word_lower = target_word.lower()\n",
        "  target_word = target_word_lower\n",
        "  try:\n",
        "    target_index = vocab[target_word]\n",
        "    target_tensor = torch.tensor([target_index],dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "      output = trained_skipgram(target_tensor)\n",
        "      top_k=5\n",
        "      top_k_indices = torch.topk(output,top_k,dim=1).indices.squeeze().tolist()\n",
        "      predicted_context_words = [words[idx] for idx in top_k_indices]\n",
        "      print(f\"Target Word: {target_word}, Predicted COntext Words (Top {top_k}): {predicted_context_words}\")\n",
        "\n",
        "  except KeyError:\n",
        "    print(\"One or more context words not found in vocabullary.\")\n",
        "\n",
        "  return {\n",
        "      'pytorch_cbow': trained_cbow,\n",
        "      'pytorch_skipgram': trained_skipgram,\n",
        "      'vocab': vocab,\n",
        "      'words': words,\n",
        "      'tkenizer': tokenizer,\n",
        "      'vocab_size': vocab_size,\n",
        "      'embedding_dim': embedding_dim\n",
        "  }"
      ],
      "metadata": {
        "id": "gNqUU1rvbMrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main** function"
      ],
      "metadata": {
        "id": "VATbQlZ0ugKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  models = run_demo(use_larger_dataset=False)\n",
        "\n",
        "  #For larger datasets\n",
        "  #models = run_demo(use_larger_dataset=True)"
      ],
      "metadata": {
        "id": "_nPSdeLEufjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34281871-67b2-4d62-c80e-77c5b691bdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch Models Training\n",
            " Training PyTorch CBOW Model\n",
            "Epoch 1/10, Loss: 3.9247\n",
            "Epoch 2/10, Loss: 3.7594\n",
            "Epoch 3/10, Loss: 3.6118\n",
            "Epoch 4/10, Loss: 3.4685\n",
            "Epoch 5/10, Loss: 3.3268\n",
            "Epoch 6/10, Loss: 3.1885\n",
            "Epoch 7/10, Loss: 3.0512\n",
            "Epoch 8/10, Loss: 2.9181\n",
            "Epoch 9/10, Loss: 2.7846\n",
            "Epoch 10/10, Loss: 2.6529\n",
            " Training PyTorch Skip-gram model\n",
            "Epoch 1/10, Loss: 4.1147\n",
            "Epoch 2/10, Loss: 3.7085\n",
            "Epoch 3/10, Loss: 3.4131\n",
            "Epoch 4/10, Loss: 3.1580\n",
            "Epoch 5/10, Loss: 2.9318\n",
            "Epoch 6/10, Loss: 2.7332\n",
            "Epoch 7/10, Loss: 2.5584\n",
            "Epoch 8/10, Loss: 2.4014\n",
            "Epoch 9/10, Loss: 2.2603\n",
            "Epoch 10/10, Loss: 2.1392\n",
            "Model Architecture and Embeddings\n",
            " CBOW Model Architecture\n",
            "CBOWModel(\n",
            "  (embeddings): Embedding(51, 20)\n",
            "  (linear): Linear(in_features=20, out_features=51, bias=True)\n",
            ")\n",
            " Skip-gram Model Embeddings\n",
            "SkipGram(\n",
            "  (embeddings): Embedding(51, 20)\n",
            "  (linear): Linear(in_features=20, out_features=51, bias=True)\n",
            ")\n",
            "Words: ['lower', 'newer']\n",
            "Shape: torch.Size([2, 20])\n",
            "Sample Values: tensor([[ 0.9419,  0.2819, -0.2717, -0.5034, -0.3566,  1.0748,  0.0487, -1.6052,\n",
            "         -0.5058, -0.7171,  0.4369,  0.2213,  1.7913,  2.4109, -0.5603, -0.4122,\n",
            "         -0.4873,  0.0604,  0.2017, -1.0385],\n",
            "        [-0.1854, -1.2936, -1.4039,  1.2920,  1.3537, -1.6065, -1.8622,  0.2753,\n",
            "          0.4505, -0.1707, -0.0663, -0.3886, -1.4347, -3.0318, -1.2869,  1.5324,\n",
            "         -0.7423,  0.3936,  0.3570, -2.2038]], requires_grad=True)\n",
            "Words: ['lower', 'newer']\n",
            "Shape: torch.Size([2, 20])\n",
            "Sample Values: tensor([[-1.0865,  1.1919,  0.9896, -0.6641, -1.2698, -1.3193,  0.3749, -0.0172,\n",
            "         -0.9299,  0.8931, -0.3154, -1.2705, -0.2000, -0.2726,  0.3795, -0.3532,\n",
            "          0.4201, -0.6078,  0.2653, -0.2703],\n",
            "        [-0.9567, -1.0456,  1.8784, -0.6427,  1.5254,  0.5656,  1.0090,  0.2342,\n",
            "          1.0260,  0.7835, -0.7931, -0.4621, -0.8767, -0.6559, -0.2636, -0.8449,\n",
            "         -0.0705, -0.4069, -0.3060,  1.9950]], requires_grad=True)\n",
            "CBOW Prediction:\n",
            "Context: ['word', 'vectors', 'semantic', 'between'], Predicted Target Word: capture\n",
            "Skip-gram Prediction:\n",
            "Target Word: capture, Predicted COntext Words (Top 5): ['ra', 'semantic', 'vectors', 'word', '##s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using **pre**-**built** **Embedding** **Models** to measure semantic similarity"
      ],
      "metadata": {
        "id": "JhRsh6dZsb1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install numpy==1.24.3\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "FMRKN5P4sbW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "q8-ZPhDsvxFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Word2Vec Google News model...\")\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def show_word2vec_relationships():\n",
        "  words = ['king','queen','man','woman','bright','dark','light']\n",
        "  print(\"\\n--- Word2Vec Embeddings ---\")\n",
        "  for word in words:\n",
        "    if word in w2v_model:\n",
        "      print(f\"{word}: {w2v_model[word][:10]}..\")\n",
        "\n",
        "  print(\"\\n--- Word2Vec Relationships ---\")\n",
        "  print(\"Similarity (king,queen): \", w2v_model.similarity('king','queen'))\n",
        "  print(\"Similarity (man,woman): \", w2v_model.similarity('man','woman'))\n",
        "\n",
        "  result = w2v_model.most_similar(positive=['woman','king'],negative=['man'])\n",
        "\n",
        "  print(\"\\n Word2Vec Analogy (king-man+woman):\")\n",
        "  print(result[:5])\n",
        "\n",
        "show_word2vec_relationships()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYuPc8jAvrY8",
        "outputId": "4f70a68e-b465-42c3-cbfe-3aefeb471098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Word2Vec Google News model...\n",
            "\n",
            "--- Word2Vec Embeddings ---\n",
            "king: [ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477 -0.03613281\n",
            "  0.11181641 -0.19824219  0.05126953  0.36328125]..\n",
            "queen: [ 0.00524902 -0.14355469 -0.06933594  0.12353516  0.13183594 -0.08886719\n",
            " -0.07128906 -0.21679688 -0.19726562  0.05566406]..\n",
            "man: [ 0.32617188  0.13085938  0.03466797 -0.08300781  0.08984375 -0.04125977\n",
            " -0.19824219  0.00689697  0.14355469  0.0019455 ]..\n",
            "woman: [ 0.24316406 -0.07714844 -0.10302734 -0.10742188  0.11816406 -0.10742188\n",
            " -0.11425781  0.02563477  0.11181641  0.04858398]..\n",
            "bright: [-0.01586914  0.12353516  0.06640625 -0.0546875   0.18164062 -0.2421875\n",
            "  0.12255859 -0.28710938 -0.08203125  0.08837891]..\n",
            "dark: [ 0.12109375  0.14550781  0.14550781 -0.20605469  0.04711914 -0.01867676\n",
            "  0.03588867 -0.20507812  0.19824219  0.15429688]..\n",
            "light: [ 0.12988281  0.17382812  0.10302734 -0.25195312  0.04003906 -0.09130859\n",
            "  0.08984375 -0.15429688  0.04589844  0.08007812]..\n",
            "\n",
            "--- Word2Vec Relationships ---\n",
            "Similarity (king,queen):  0.6510957\n",
            "Similarity (man,woman):  0.76640123\n",
            "\n",
            " Word2Vec Analogy (king-man+woman):\n",
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581)]\n"
          ]
        }
      ]
    }
  ]
}